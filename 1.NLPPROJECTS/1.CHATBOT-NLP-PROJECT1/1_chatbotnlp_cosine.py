# -*- coding: utf-8 -*-
"""1.CHATBOTNLP-COSINE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EoHEKoOk_sWTEWHD_xLOy0tTBwcD_4j8


## **Problem statements:**
- This project is a **simple chatbot that uses cosine similarit**y for question answering. It performs tokenization and stopword removal on the user's input, then matches the input to a pre-defined list of questions using cosine similarity. If a match is found, the corresponding answer is returned to the user. If no match is found, the chatbot responds with "We can't answer this".
- Need:  Python 3.x
  - nltk
  - numpy
"""

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd

path= r"/content/drive/MyDrive/IMP1DS INTERVIEW PREP2024/15.DSPROJECT2024/1.NLPPROJECTS2024/test.csv"
df = pd.read_csv(path, encoding='unicode_escape')

questions_list = df['Questions'].tolist()

questions_list

answers_list=df['Answers'].tolist()

answers_list

import nltk
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

nltk.download('punkt')
nltk.download('wordnet')
nltk.download('stopwords')

"""### nltk.download('punkt'):
- Downloads the Punkt tokenizer, which is used for sentence splitting and word tokenization.

### nltk.download('wordnet')
-  Downloads the WordNet lexical database, used for finding word meanings, synonyms, antonyms, etc.

### nltk.download('stopwords')
- Downloads a list of common stopwords (like "and", "the", "is") that can be excluded from text analysis.
"""

from nltk.stem import WordNetLemmatizer, PorterStemmer
from nltk.corpus import stopwords
import re

def preprocess(text):
    lemmatizer = WordNetLemmatizer()
    stemmer = PorterStemmer()
    text = re.sub(r'[^\w\s]', '', text)  # Remove non-alphanumeric characters
    tokens = nltk.word_tokenize(text.lower())
    tokens = [token for token in tokens if token not in stopwords.words('english')]
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]
    return ' '.join(stemmed_tokens)
 #not using this code

'''# The stopwords list is not used in this code because excluding stopwords would make some questions identical.
The stopwords list is not used in this code because removing common words like "the" and "and" could make some questions appear identical, which would affect the accuracy of the responses.
'''

#using this code :
def preprocess_with_stopwords(text):
    lemmatizer = WordNetLemmatizer()
    stemmer = PorterStemmer()
    text = re.sub(r'[^\w\s]', '', text)  # Remove non-alphanumeric characters,PUNCTUATIONS
    tokens = nltk.word_tokenize(text.lower()) #lowercase
    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]
    stemmed_tokens = [stemmer.stem(token) for token in lemmatized_tokens]
    return ' '.join(stemmed_tokens)

"""- Initialize Tools: Sets up a lemmatizer and a stemmer for processing words.
- Remove Punctuation: Removes non-alphanumeric characters (like punctuation) from the text.
- Tokenize: Breaks the text into individual words, converting them to lowercase.
- **Lemmatize**: Reduces each word to its base or root form using the lemmatizer.
- Stem: Further reduces each lemmatized word to its root form using the stemmer.
- Join Tokens: Combines the processed words back into a single string.
-- **Lemmatize Tokens**: Convert each word to its base form.
-- **Stem Tokens**: Reduce each lemmatized word to its root form.
-- Jo**in Tokens**: Combine all processed words into one string.
- - Lemmatize Tokens: Change "Running" and "Ran" to "run", making the sentence simpler.

-- Input: "Running runners ran quickly."
Output: ["run", "runner", "run", "quickly"]
- - Stem Tokens: Further reduce the words to their roots.

- Input: ["run", "runner", "run", "quickly"]
Output: ["run", "runner", "run", "quick"]
- Base words: The basic, dictionary form of a word.
- Root Form: The core part of a word after removing prefixes or suffixes
"""

import warnings
# Suppress warnings
warnings.filterwarnings("ignore")

# Setup Vectorizer: Initializes TF-IDF vectorizer to use word tokenization.
# Transform Data: Converts preprocessed questions into TF-IDF vectors.

vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)
X = vectorizer.fit_transform([preprocess(q) for q in questions_list])

def get_response(text):
    processed_text = preprocess_with_stopwords(text)
    print("processed_text:", processed_text)
    vectorized_text = vectorizer.transform([processed_text])
    similarities = cosine_similarity(vectorized_text, X)
    print("similarities:", similarities)
    max_similarity = np.max(similarities)
    print("max_similarity:", max_similarity)
    if max_similarity > 0.6:
        high_similarity_questions = [q for q, s in zip(questions_list, similarities[0]) if s > 0.6]
        print("high_similarity_questions:", high_similarity_questions)

        target_answers = []
        for q in high_similarity_questions:
            q_index = questions_list.index(q)
            target_answers.append(answers_list[q_index])
        print(target_answers)
        Z = vectorizer.fit_transform([preprocess_with_stopwords(q) for q in high_similarity_questions])
        processed_text_with_stopwords = preprocess_with_stopwords(text)
        print("processed_text_with_stopwords:", processed_text_with_stopwords)
        vectorized_text_with_stopwords = vectorizer.transform([processed_text_with_stopwords])
        final_similarities = cosine_similarity(vectorized_text_with_stopwords, Z)
        closest = np.argmax(final_similarities)
        return target_answers[closest]
    else:
        return "I can't answer this question."
get_response('Who is ms dhoni?')

"""- Process: Clean the input text.
- Vectorize: Convert text to numerical form.
- Compare: Measure similarity to preprocessed questions.(>0.6 isexact match)
-Select: Choose the best matching answer or return a default message
"""

get_response('Who is ms dhoni?')

get_response('What is a machine learning')

!pip install gingerit

from gingerit.gingerit import GingerIt

text = 'What is Data Anlytics'

parser = GingerIt()
corrected_text = parser.parse(text)

print(corrected_text['result'])

!pip install textblob

#if ginegerbit not working use textblot for checking spelling

from textblob import TextBlob

text = 'What is Data Anlytics'
blob = TextBlob(text)

corrected_text = blob.correct()
print(corrected_text)